{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccce3f57-90d6-40cd-b1fc-8d6ef649181f",
   "metadata": {},
   "source": [
    "# Class-based version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f185259f-fafe-47f8-a3ad-3f9883825f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Starting scrape...\n",
      "[Info] Fetching page 1...\n",
      "[Info] Fetching page 2...\n",
      "[Info] Fetching page 3...\n",
      "[Info] Fetching page 4...\n",
      "[Info] Fetching page 5...\n",
      "[Info] Fetching page 6...\n",
      "[Info] Fetching page 7...\n",
      "[Info] Fetching page 8...\n",
      "[Info] Fetching page 9...\n",
      "[Info] Fetching page 10...\n",
      "[Info] Fetching page 11...\n",
      "[Info] No more quotes found. Stopping.\n",
      "[Info] Quotes saved to quotes.csv\n",
      "[Info] Quotes saved to quotes.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "class QuoteScraper:\n",
    "    \"\"\"\n",
    "    Scrapes quotes, authors, and tags from http://quotes.toscrape.com/\n",
    "    and saves the results in both CSV and JSON formats.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url='http://quotes.toscrape.com/page/'):\n",
    "        self.base_url = base_url\n",
    "        self.quotes = []\n",
    "\n",
    "    def fetch_page(self, page_number):\n",
    "        \"\"\"\n",
    "        Fetches the content of a given page number. Returns BeautifulSoup object or None on failure.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}{page_number}/\", timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[Error] Could not fetch page {page_number}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_quotes(self, soup):\n",
    "        \"\"\"\n",
    "        Parses all quotes on a given page soup and appends to the quotes list.\n",
    "        \"\"\"\n",
    "        quote_divs = soup.find_all('div', class_='quote')\n",
    "        for div in quote_divs:\n",
    "            text = div.find('span', class_='text').get_text(strip=True)\n",
    "            author = div.find('small', class_='author').get_text(strip=True)\n",
    "            tags = [tag.get_text(strip=True) for tag in div.find_all('a', class_='tag')]\n",
    "            self.quotes.append({\n",
    "                'quote': text,\n",
    "                'author': author,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "    def scrape_all_pages(self):\n",
    "        \"\"\"\n",
    "        Loops through all paginated pages until no more quotes are found.\n",
    "        \"\"\"\n",
    "        print(\"[Info] Starting scrape...\")\n",
    "        page = 1\n",
    "        while True:\n",
    "            print(f\"[Info] Fetching page {page}...\")\n",
    "            soup = self.fetch_page(page)\n",
    "            if not soup:\n",
    "                break\n",
    "            if not soup.find_all('div', class_='quote'):\n",
    "                print(\"[Info] No more quotes found. Stopping.\")\n",
    "                break\n",
    "            self.parse_quotes(soup)\n",
    "            page += 1\n",
    "            time.sleep(1)  # Be polite: add a short delay\n",
    "\n",
    "    def save_to_csv(self, filename='quotes.csv'):\n",
    "        \"\"\"\n",
    "        Saves the scraped quotes to a CSV file.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.quotes)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"[Info] Quotes saved to {filename}\")\n",
    "\n",
    "    def save_to_json(self, filename='quotes.json'):\n",
    "        \"\"\"\n",
    "        Saves the scraped quotes to a JSON file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.quotes, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"[Info] Quotes saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = QuoteScraper()\n",
    "    scraper.scrape_all_pages()\n",
    "    scraper.save_to_csv()\n",
    "    scraper.save_to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99193def-10b6-4b12-8b29-3d35ac65ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('quotes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a660394-56c5-45d0-a810-bd0d8d3b3e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“The world as we have created it is a process ...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>['change', 'deep-thoughts', 'thinking', 'world']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“It is our choices, Harry, that show what we t...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>['abilities', 'choices']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“There are only two ways to live your life. On...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>['inspirational', 'life', 'live', 'miracle', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The person, be it gentleman or lady, who has ...</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>['aliteracy', 'books', 'classic', 'humor']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Imperfection is beauty, madness is genius and...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>['be-yourself', 'inspirational']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote           author  \\\n",
       "0  “The world as we have created it is a process ...  Albert Einstein   \n",
       "1  “It is our choices, Harry, that show what we t...     J.K. Rowling   \n",
       "2  “There are only two ways to live your life. On...  Albert Einstein   \n",
       "3  “The person, be it gentleman or lady, who has ...      Jane Austen   \n",
       "4  “Imperfection is beauty, madness is genius and...   Marilyn Monroe   \n",
       "\n",
       "                                                tags  \n",
       "0   ['change', 'deep-thoughts', 'thinking', 'world']  \n",
       "1                           ['abilities', 'choices']  \n",
       "2  ['inspirational', 'life', 'live', 'miracle', '...  \n",
       "3         ['aliteracy', 'books', 'classic', 'humor']  \n",
       "4                   ['be-yourself', 'inspirational']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6aa8e4-03f8-4395-b55a-c826813c5a82",
   "metadata": {},
   "source": [
    "# Function-based version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e145a71-6dc3-4022-bbe8-63477d17b567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Fetching page 1...\n",
      "[Info] Fetching page 2...\n",
      "[Info] Fetching page 3...\n",
      "[Info] Fetching page 4...\n",
      "[Info] Fetching page 5...\n",
      "[Info] Fetching page 6...\n",
      "[Info] Fetching page 7...\n",
      "[Info] Fetching page 8...\n",
      "[Info] Fetching page 9...\n",
      "[Info] Fetching page 10...\n",
      "[Info] Fetching page 11...\n",
      "[Info] Quotes saved to quotes.csv\n",
      "[Info] Quotes saved to quotes.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "BASE_URL = 'http://quotes.toscrape.com/page/'\n",
    "\n",
    "def fetch_page(page_number):\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}{page_number}/\", timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"[Error] Could not fetch page {page_number}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_quotes(soup):\n",
    "    quotes = []\n",
    "    quote_divs = soup.find_all('div', class_='quote')\n",
    "    for div in quote_divs:\n",
    "        text = div.find('span', class_='text').get_text(strip=True)\n",
    "        author = div.find('small', class_='author').get_text(strip=True)\n",
    "        tags = [tag.get_text(strip=True) for tag in div.find_all('a', class_='tag')]\n",
    "        quotes.append({'quote': text, 'author': author, 'tags': tags})\n",
    "    return quotes\n",
    "\n",
    "def scrape_all_pages():\n",
    "    all_quotes = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print(f\"[Info] Fetching page {page}...\")\n",
    "        soup = fetch_page(page)\n",
    "        if not soup or not soup.find_all('div', class_='quote'):\n",
    "            break\n",
    "        quotes = parse_quotes(soup)\n",
    "        all_quotes.extend(quotes)\n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "    return all_quotes\n",
    "\n",
    "def save_to_csv(quotes, filename='quotes.csv'):\n",
    "    df = pd.DataFrame(quotes)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[Info] Quotes saved to {filename}\")\n",
    "\n",
    "def save_to_json(quotes, filename='quotes.json'):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(quotes, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"[Info] Quotes saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quotes = scrape_all_pages()\n",
    "    save_to_csv(quotes)\n",
    "    save_to_json(quotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816db1a-48d1-467a-bfce-971e3ff22970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
